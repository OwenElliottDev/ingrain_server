{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Ingrain is a wrapper for Triton Inference Server that makes using it with sentence transformers and open CLIP models easy.</p> <p>To use: <pre><code>docker run --name ingrain_server -p 8686:8686 -p 8687:8687 --gpus all owenpelliott/ingrain-server:latest\n</code></pre></p> <p>To run without a GPU remove the <code>--gpus all</code> flag.</p>"},{"location":"#usage","title":"Usage","text":"<p>The serve is available via a REST API, there is also a Python client available.</p> <pre><code>pip install ingrain\n</code></pre>"},{"location":"#what-does-it-do","title":"What does it do?","text":"<p>This server handles all the model loading, ONNX conversion, memory management, parallelisation, dynamic batching, input pre-processing, image handling, and other complexities of running a model in production. The API is very simple but lets you serve models in a performant manner.</p> <p>Open CLIP models and sentence transformers are both converted to ONNX and served by Triton. The server can handle multiple models at once.</p>"},{"location":"#how-does-it-perform","title":"How does it perform?","text":"<p>It retains all the performance of Triton. On 12 cores at 4.3 GHz with a 2080 SUPER 8GB card running in Docker using WSL2, it can serve <code>intfloat/e5-small-v2</code> to 500 clients at ~1050 QPS, or <code>intfloat/e5-base-v2</code> to 500 clients at ~860 QPS.</p>"},{"location":"#how-compatible-is-it","title":"How compatible is it?","text":"<p>Most models work out of the box, it is intractable to test every sentence transformers model and every CLIP models but most main architectures are tested and work. If you have a model that doesn't work, please open an issue.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Ingrain is a scalable wrapper for Triton Inference Server that makes using it with sentence transformers, timm, and open CLIP models easy.</p> <p>The server handles all model conversion, loading, and preprocessing. It manages tokenizers, image downloading, image preprocessing, and other complexities of running a model. </p>"},{"location":"getting_started/#setup","title":"Setup","text":"<p>To run Ingrain, you need to have Docker installed. You can run the server with the following command:</p> <pre><code>docker run --name ingrain_server -p 8686:8686 -p 8687:8687 --gpus all owenpelliott/ingrain-server:latest\n</code></pre> <p>To run without a GPU, remove the <code>--gpus all</code> flag.</p> <p>You can also install a python client to get started quickly:</p> <pre><code>pip install ingrain\n</code></pre>"},{"location":"getting_started/#example-usage","title":"Example Usage","text":"<p>Ingrain has two main components: the model server and the inference server. The model server handles all model loading and unloading operations whereas the inference server handles all inference requests on loaded models.</p>"},{"location":"getting_started/#loading-models","title":"Loading Models","text":"<p>Models are accessable via the same naming conventions as their underlying libraries (<code>open_clip_torch</code>, <code>sentence_transformers</code>, or <code>timm</code>). For example to load a MobileCLIP model in <code>open_clip</code> you would use the following code:</p> <pre><code>import open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('MobileCLIP-S2', pretrained='datacompdr')\nmodel.eval()\ntokenizer = open_clip.get_tokenizer('MobileCLIP-S2')\n</code></pre> <p>To do the same in Ingrain, you would use the following code:</p> <pre><code>import ingrain\n\nclient = ingrain.Client()\nclient.load_clip_model(name=\"MobileCLIP-S2\", pretrained=\"datacompdr\")\n</code></pre> <p>A similar pattern holds for <code>sentence_transformers</code> models:</p> <p>Sentence Transformers:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('intfloat/e5-small-v2')\n</code></pre> <p>Ingrain:</p> <pre><code>import ingrain\n\nclient = ingrain.Client()\nclient.load_sentence_transformer_model(name=\"intfloat/e5-small-v2\")\n</code></pre>"},{"location":"getting_started/#inference","title":"Inference","text":"<p>Inference requests are made to the Inference Server. The server can handle both text and image inputs. Images can be URLs to image assets or base64 encoded images. Requests can include single items or batches of items - the server will also dynamically batch requests to optimize performance.</p> <p>For example to encode a list of sentences using the <code>intfloat/e5-small-v2</code> model: <pre><code>import ingrain\n\nclient = ingrain.Client()\nclient.load_sentence_transformer_model(name=\"intfloat/e5-small-v2\")\n\nresponse = client.infer(\n    name=\"intfloat/e5-small-v2\", \n    text=[\"I am a sentence.\", \"I am another sentence.\", \"I am a third sentence.\"]\n)\n</code></pre></p> <p>This will return a dictionary like so:</p> <pre><code>{\n    'textEmbeddings': [\n        [-0.028349684551358223, 0.03690376505255699, ...],\n        [-0.657342546546734223, 0.46323376505255699, ...],\n        [-0.035745636365643294, 0.95440376505255699, ...]\n    ], \n    'imageEmbeddings': None, \n    'processingTimeMs': 17.091903000618913\n}\n</code></pre>"},{"location":"inference_server/","title":"Inference Server API Documentation","text":"<p>This section documents the endpoints available on the Inference Server.</p>"},{"location":"inference_server/#endpoints","title":"Endpoints","text":""},{"location":"inference_server/#get-health","title":"GET <code>/health</code>","text":"<p>Returns a JSON message confirming that the inference server is running.</p> cURL <pre><code>curl -X GET \"http://127.0.0.1:8686/health\" -H \"accept: application/json\"\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nclient.health()\n</code></pre>"},{"location":"inference_server/#post-infer_text","title":"POST <code>/infer_text</code>","text":"<p>Performs text inference using the specified model.</p> <p>Request Body:</p> <ul> <li><code>name</code>: Model name.</li> <li><code>pretrained</code>: Pretrained checkpoint identifier. Not used for Sentence Transformers models.</li> <li><code>text</code>: Text input for inference. Can be a list to infer a batch.</li> <li><code>normalize</code>: Boolean flag for normalization.</li> <li><code>n_dims</code>: Number of dimensions for the embedding. Only useful for MRL models.</li> </ul> cURL <pre><code>curl -X POST \"http://127.0.0.1:8686/infer_text\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\": \"intfloat/e5-small-v2\", \"pretrained\": null, \"text\": \"sample text\", \"normalize\": true, \"n_dims\": null}'\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\n\nresponse = client.infer_text(\n    name=\"intfloat/e5-small-v2\", \n    text=\"sample text\", \n)\n</code></pre>"},{"location":"inference_server/#post-infer_image","title":"POST <code>/infer_image</code>","text":"<p>Performs image inference using the specified model.</p> <p>Request Body:</p> <ul> <li><code>name</code>: Model name.</li> <li><code>pretrained</code>: Pretrained checkpoint identifier.</li> <li><code>image</code>: Image URL or base64 encoded image. Can be a list to infer a batch.</li> <li><code>normalize</code>: Boolean flag for normalization.</li> <li><code>n_dims</code>: Number of dimensions for the embedding. Only useful for MRL models.</li> <li><code>image_download_headers</code>: Optional headers for image downloading requests.</li> </ul> cURL <pre><code>curl -X POST \"http://127.0.0.1:8686/infer_image\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\": \"MobileCLIP-S2\", \"pretrained\": \"datacompdr\", \"image\": \"http://example.com/image.jpg\", \"normalize\": true, \"n_dims\": null}'\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nresponse = client.infer_image(\n    name=\"MobileCLIP-S2\", \n    pretrained=\"datacompdr\", \n    image=\"http://example.com/image.jpg\"\n)\n</code></pre>"},{"location":"inference_server/#post-infer","title":"POST <code>/infer</code>","text":"<p>Performs both text and image inference concurrently, can do either or both together.</p> <p>Request Body:</p> <ul> <li><code>name</code>: Model name.</li> <li><code>pretrained</code>: Pretrained checkpoint identifier.</li> <li><code>text</code>: Text input for inference. Can be a list to infer a batch.</li> <li><code>image</code>: Image URL or base64 encoded image. Can be a list to infer a batch.</li> <li><code>normalize</code>: Boolean flag for normalization.</li> <li><code>n_dims</code>: Number of dimensions for the embedding. Only useful for MRL models.</li> <li><code>image_download_headers</code>: Optional headers for image downloading.</li> </ul> cURL <pre><code>curl -X POST \"http://127.0.0.1:8686/infer\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"\"name\": \"MobileCLIP-S2\", \"pretrained\": \"datacompdr\", \"text\": \"sample text\", \"image\": \"http://example.com/image.jpg\", \"normalize\": true, \"n_dims\": 512}'\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nresponse = client.infer_image(\n    name=\"MobileCLIP-S2\", \n    pretrained=\"datacompdr\", \n    image=\"http://example.com/image.jpg\", \n    text=\"sample text\"\n)\n</code></pre>"},{"location":"inference_server/#get-metrics","title":"GET <code>/metrics</code>","text":"<p>Retrieves server metrics.</p> cURL <pre><code>curl -X GET \"http://127.0.0.1:8686/metrics\" -H \"accept: application/json\"\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\n\nmetrics = client.metrics()\n</code></pre>"},{"location":"model_server/","title":"Model Server API Documentation","text":"<p>This section documents the endpoints available on the Model Server.</p>"},{"location":"model_server/#endpoints","title":"Endpoints","text":""},{"location":"model_server/#get-health","title":"GET <code>/health</code>","text":"<p>Returns a JSON message confirming that the model server is running.</p> cURL <pre><code>curl -X GET \"http://127.0.0.1:8687/health\" -H \"accept: application/json\"\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nclient.health()\n</code></pre>"},{"location":"model_server/#post-load_clip_model","title":"POST <code>/load_clip_model</code>","text":"<p>Loads a CLIP model.</p> <p>Request Body:</p> <ul> <li><code>name</code>: Model name.</li> <li><code>pretrained</code>: Pretrained checkpoint identifier.</li> </ul> cURL <pre><code>curl -X POST \"http://127.0.0.1:8687/load_clip_model\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\": \"MobileCLIP-S2\", \"pretrained\": \"datacompdr\"}'\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\n\nclient.load_clip_model(name=\"MobileCLIP-S2\", pretrained=\"datacompdr\")\n</code></pre>"},{"location":"model_server/#post-load_sentence_transformer_model","title":"POST <code>/load_sentence_transformer_model</code>","text":"<p>Loads a Sentence Transformer model.</p> <p>Request Body:</p> <ul> <li><code>name</code>: Model name.</li> </ul> cURL <pre><code>curl -X POST \"http://127.0.0.1:8687/load_sentence_transformer_model\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\": \"intfloat/e5-small-v2\"}'\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nclient.load_sentence_transformer_model(name=\"intfloat/e5-small-v2\")\n</code></pre>"},{"location":"model_server/#post-load_timm_model","title":"POST <code>/load_timm_model</code>","text":"<p>Loads a Timm model.</p> <p>Request Body:</p> <ul> <li><code>name</code>: Model name.</li> <li><code>pretrained</code>: Pretrained, <code>true</code> or <code>false</code> - defaults to <code>true</code>. There is no clear use case for setting this to <code>false</code>.</li> </ul> cURL <pre><code>curl -X POST \"http://127.0.0.1:8687/load_timm_model\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\": \"hf_hub:timm/mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k\", \"pretrained\": true}'\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nclient.load_timm_model(name=\"hf_hub:timm/mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k\")\n</code></pre>"},{"location":"model_server/#post-unload_model","title":"POST <code>/unload_model</code>","text":"<p>Unloads a model from the server.</p> <p>Request Body:</p> <ul> <li><code>name</code>: Model name.</li> <li><code>pretrained</code>: Pretrained checkpoint identifier. Not used for Sentence Transformers models.</li> </ul> cURL <pre><code>curl -X POST \"http://127.0.0.1:8687/unload_model\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\": \"intfloat/e5-small-v2\", \"pretrained\": null}'\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nclient.unload_model(name=\"intfloat/e5-small-v2\")\n</code></pre>"},{"location":"model_server/#delete-delete_model","title":"DELETE <code>/delete_model</code>","text":"<p>Deletes a model from the repository.</p> <p>Request Body:</p> <ul> <li><code>name</code>: Model name.</li> <li><code>pretrained</code>: Pretrained checkpoint identifier. Not used for Sentence Transformers models.</li> </ul> cURL <pre><code>curl -X DELETE \"http://127.0.0.1:8687/delete_model\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\": \"intfloat/e5-small-v2\", \"pretrained\": null}'\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nclient.delete_model(name=\"intfloat/e5-small-v2\")\n</code></pre>"},{"location":"model_server/#get-loaded_models","title":"GET <code>/loaded_models</code>","text":"<p>Retrieves a list of currently loaded models.</p> cURL <pre><code>curl -X GET \"http://127.0.0.1:8687/loaded_models\" -H \"accept: application/json\"\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nresp = client.loaded_models()\n</code></pre>"},{"location":"model_server/#get-repository_models","title":"GET <code>/repository_models</code>","text":"<p>Retrieves a list of models in the repository.</p> cURL <pre><code>curl -X GET \"http://127.0.0.1:8687/repository_models\" -H \"accept: application/json\"\n</code></pre> Python <pre><code>import ingrain\n\nclient = ingrain.Client()\nresp = client.repository_models()\n</code></pre>"},{"location":"model_server/#post-download_custom_model","title":"POST <code>/download_custom_model</code>","text":"<p>Experimental, subject to change. Downloads a custom model.</p> <p>Request Body:</p> <ul> <li><code>library</code>: The model library type (<code>open_clip</code>, <code>sentence_transformers</code>, or <code>timm</code>).</li> <li><code>pretrained_name</code>: The name of the pretrained model.</li> <li><code>safetensors_url</code>: URL to the model file.</li> <li>Other parameters may be required based on the library.</li> </ul> cURL <pre><code>curl -X POST \"http://127.0.0.1:8687/download_custom_model\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"library\": \"open_clip\", \"pretrained_name\": \"your_model\", \"safetensors_url\": \"http://example.com/model.safetensors\"}'\n</code></pre> Python <pre><code># N/A\n</code></pre>"}]}