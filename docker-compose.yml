version: "3.9"

services:
  ingrain:
    build: .
    container_name: ingrain
    ports:
      - "8686:8686"
      - "8687:8687"
    environment:
      TRITON_GRPC_URL: triton:8001
    depends_on:
      - triton
    volumes:
      - ./model_repository:/app/model_repository 
      - ${HOME}/.cache/huggingface:/app/model_cache/
  triton:
    image: nvcr.io/nvidia/tritonserver:25.04-py3
    container_name: triton
    # runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - LD_PRELOAD=/usr/lib/$(uname -m)-linux-gnu/libtcmalloc.so.4
    shm_size: "1g"
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    volumes:
      - ./model_repository:/models
    restart:
      always
