services:
  ingrain:
    build: .
    container_name: ingrain
    ports:
      - "8686:8686"
      - "8687:8687"
    environment:
      - TRITON_GRPC_URL=triton:8001
      - MAX_BATCH_SIZE=16
      - MODEL_INSTANCES=1
      - INSTANCE_KIND=KIND_GPU
    depends_on:
      - triton
    volumes:
      - ./model_repository:/app/model_repository 
      - ./model_cache:/app/model_cache/
  triton:
    image: nvcr.io/nvidia/tritonserver:25.04-py3
    container_name: triton
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    shm_size: "256m"
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    volumes:
      - ./model_repository:/models
    restart:
      always
