services:
  ingrain-models:
    build: 
      context: ./ingrain_model_server
      dockerfile: Dockerfile
    container_name: ingrain-models
    ports:
      - "8687:8687"
    environment:
      - TRITON_GRPC_URL=triton:8001
      - MAX_BATCH_SIZE=16
      - MODEL_INSTANCES=1
      - INSTANCE_KIND=KIND_CPU
    depends_on:
      - triton
    volumes:
      - ./model_repository:/app/model_repository 
      - ./model_cache:/app/model_cache/
  ingrain-inference:
    build: 
      context: ./ingrain_inference_server
      dockerfile: Dockerfile
    container_name: ingrain-inference
    ports:
      - "8686:8686"
    environment:
      - TRITON_GRPC_URL=triton:8001
    depends_on:
      - triton
    volumes:
      - ./model_repository:/app/model_repository 
      - ./model_cache:/app/model_cache/
  triton:
    image: nvcr.io/nvidia/tritonserver:25.06-py3
    container_name: triton
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    shm_size: "256m"
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    volumes:
      - ./model_repository:/models
    restart:
      always
